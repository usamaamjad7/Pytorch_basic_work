{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Building_a_neural_network_using_PyTorch_on_a_toy_dataset.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPThsn+eRV8d8e7oggOY/0s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0HdffBL3XMv1"},"source":["# **Building a Neural Network using pytorch**"]},{"cell_type":"markdown","metadata":{"id":"gLHDu6h1XBYS"},"source":["However, for all of these, we built them from scratch using NumPy arrays in Python. In this section, we will learn about implementing all of these using PyTorch on a toy dataset. Note that we will leverage our learning so far regarding initializing tensor objects, performing various operations on top of them, and calculating the gradient values to update weights when building a neural network using PyTorch."]},{"cell_type":"markdown","metadata":{"id":"mHaGUpgOXE7n"},"source":["Note that, in this chapter, to gain the intuition of performing various operations, we will build a neural network on a toy dataset. Starting with the next chapter, we will deal with solving more realistic problems and datasets."]},{"cell_type":"markdown","metadata":{"id":"yy_vqvr3XZi_"},"source":["The toy problem we'll solve to understand the implementation of neural networks using PyTorch is a plain addition of two numbers, where we initialize the dataset as follows:"]},{"cell_type":"markdown","metadata":{"id":"qPCkqUDdXfsi"},"source":["# **Define the input (x) and output (y) values:**"]},{"cell_type":"code","metadata":{"id":"ODroYqlFWjj-"},"source":["import torch\n","x = [[1,2],[3,4],[5,6],[7,8]]\n","y = [[3],[7],[11],[15]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6mJ5yupcYIeo"},"source":["### the input and output are a list of lists where the sum of values in the input list is the values in the output list."]},{"cell_type":"markdown","metadata":{"id":"lKYGaSUkYQLn"},"source":["# Convert the input list in to Tensor object.."]},{"cell_type":"code","metadata":{"id":"sVU0gweMX7vm"},"source":["X = torch.tensor(x).float()\n","Y = torch.tensor(y).float()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6J_h0QE8Y11-"},"source":["Note that in the preceding code, we have converted the tensor objects into floating-point objects. It is good practice to have tensor objects as floats or long ints, as they will be multiplied by decimal values (weights) anyway."]},{"cell_type":"markdown","metadata":{"id":"2YEUpL-5Y4LV"},"source":["# Furthermore, we register the input (X) and output (Y) data points to the device – cuda if you have a GPU and cpu "]},{"cell_type":"code","metadata":{"id":"KLYggDpAYnFu"},"source":["device = 'cuda' if torch.cuda.is_available() else 'gpu'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HnlVf4GqZNtj"},"source":["X = X.to(device)\n","Y = Y.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_1REXkXsZdi5"},"source":["# Define the neural network Architecture.."]},{"cell_type":"markdown","metadata":{"id":"hDueah8aZoeY"},"source":["The torch.nn module contains functions that help in building neural network models:"]},{"cell_type":"code","metadata":{"id":"pg_nph12Za5V"},"source":["import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ObaqeEJDaG04"},"source":["# We will create a class (MyNeuralNet) that can compose our neural network architecture. It is mandatory to inherit from nn.Module when creating a model architecture as it is the base class for all neural network modules:"]},{"cell_type":"code","metadata":{"id":"U8NyM8bxZ4S1"},"source":["class MyNeuralNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.input_to_hidden_layer = nn.Linear(2,8)\n","        self.hidden_layer_activation = nn.ReLU()\n","        self.hidden_to_output_layer = nn.Linear(8,1)\n","        #Now that we have defined the components of a neural network, let's connect the components together while defining the forward propagation of the network:\n","    def forward(self, x):\n","        x = self.input_to_hidden_layer(x)\n","        x = self.hidden_layer_activation(x)\n","        x = self.hidden_to_output_layer(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZ5ozCmhfIfS"},"source":["# You can access the initial weights of each of the components by performing the following steps\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f17ng4CdfY7U"},"source":["Create an instance of the MyNeuralNet class object that we defined earlier and register it to device:"]},{"cell_type":"code","metadata":{"id":"SbMBvc47bfIv"},"source":["mynet = MyNeuralNet().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hyo6lHZTfxmd"},"source":["# The weights and bias of each layer can be accessed by specifying the following:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WjLfWbNZfiNt","executionInfo":{"status":"ok","timestamp":1621794716955,"user_tz":-300,"elapsed":558,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"2c4ae537-5e44-4e55-cd81-81535c3d76d3"},"source":["# NOTE - This line of code is not a part of model building, \n","# this is used only for illustration of \n","# how to obtain parameters of a given layer\n","mynet.input_to_hidden_layer.weight"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-0.6410,  0.2607],\n","        [ 0.6687, -0.3547],\n","        [-0.5858,  0.2037],\n","        [ 0.0775,  0.6394],\n","        [ 0.4973, -0.5715],\n","        [-0.4712, -0.6182],\n","        [ 0.3971, -0.6732],\n","        [-0.6622,  0.1049]], device='cuda:0', requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"fdkuIcD6hGpm"},"source":["The values in your output will vary from the preceding, as the neural network is initialized with random values every time. If you wanted them to remain the same in multiple iterations of executing the same code, you would need to specify the seed using the manual_seed method in Torch as torch.manual_seed(0) just before creating the instance of the class object"]},{"cell_type":"markdown","metadata":{"id":"DctdK56-hMAX"},"source":["# All the parameters of a neural network can be obtained by using the following code:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QSBaQu6kf3f-","executionInfo":{"status":"ok","timestamp":1621795142986,"user_tz":-300,"elapsed":453,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"a727f669-0ffd-4388-d556-ae76ebe056b9"},"source":["# NOTE - This line of code is not a part of model building, \n","# this is used only for illustration of \n","# how to obtain parameters of a given layer\n","mynet.input_to_hidden_layer.weight"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-0.6410,  0.2607],\n","        [ 0.6687, -0.3547],\n","        [-0.5858,  0.2037],\n","        [ 0.0775,  0.6394],\n","        [ 0.4973, -0.5715],\n","        [-0.4712, -0.6182],\n","        [ 0.3971, -0.6732],\n","        [-0.6622,  0.1049]], device='cuda:0', requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"laa4dmFTiWg3"},"source":["# The preceding code returns a generator object."]},{"cell_type":"markdown","metadata":{"id":"_jD6AiUdiY6C"},"source":["# Finally, the parameters are obtained by looping through the generator, as follows:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vBurpUIfhfjo","executionInfo":{"status":"ok","timestamp":1621795423853,"user_tz":-300,"elapsed":707,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"83a1dbff-5ce9-416b-ac7c-02bbf8f3fc42"},"source":["for par in mynet.parameters():\n","    print(par)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Parameter containing:\n","tensor([[-0.6410,  0.2607],\n","        [ 0.6687, -0.3547],\n","        [-0.5858,  0.2037],\n","        [ 0.0775,  0.6394],\n","        [ 0.4973, -0.5715],\n","        [-0.4712, -0.6182],\n","        [ 0.3971, -0.6732],\n","        [-0.6622,  0.1049]], device='cuda:0', requires_grad=True)\n","Parameter containing:\n","tensor([-0.0161, -0.5582, -0.7023,  0.5704, -0.2971,  0.6022,  0.2933, -0.0413],\n","       device='cuda:0', requires_grad=True)\n","Parameter containing:\n","tensor([[-0.0750, -0.2112, -0.1910, -0.1483,  0.1690, -0.1606, -0.2302, -0.3207]],\n","       device='cuda:0', requires_grad=True)\n","Parameter containing:\n","tensor([-0.2634], device='cuda:0', requires_grad=True)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hxng-Q9Pl3nJ"},"source":["The model has registered these tensors as special objects that are necessary for keeping track of forward and backward propagation. When defining any nn layers in the __init__ method, it will automatically create corresponding tensors and simultaneously register them. You can also manually register these parameters using the nn.Parameter(<tensor>) function. Hence, the following code is equivalent to the neural network class that we defined previously."]},{"cell_type":"markdown","metadata":{"id":"UYurlWKhmA2y"},"source":["# An alternative way of defining the model using the nn.Parameter function is as follows:"]},{"cell_type":"code","metadata":{"id":"NCFKWwBvijuu"},"source":["# for illustration only\n","class MyNeuralNet(nn.Module):\n","     def __init__(self):\n","        super().__init__()\n","        self.input_to_hidden_layer = nn.Parameter(\\\n","                                          torch.rand(2,8))\n","        self.hidden_layer_activation = nn.ReLU()\n","        self.hidden_to_output_layer = nn.Parameter(\\\n","                                          torch.rand(8,1))\n","\n","     def forward(self, x):\n","        x = x @ self.input_to_hidden_layer\n","        x = self.hidden_layer_activation(x)\n","        x = x @ self.hidden_to_output_layer\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PaWUX_PLmNH3"},"source":["# Define the loss function that we optimize for. Given that we are predicting for a continuous output, we'll optimize for mean squared error:"]},{"cell_type":"code","metadata":{"id":"JznYTU3PmJcW"},"source":["loss_func = nn.MSELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ySQ_7i3mX90","executionInfo":{"status":"ok","timestamp":1621796522415,"user_tz":-300,"elapsed":418,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"b08dae93-d259-45cd-ac4f-6ad2cd70e863"},"source":["_Y = mynet(X)\n","loss_value = loss_func(_Y,Y)\n","print(loss_value)\n","# tensor(91.5550, grad_fn=<MseLossBackward>)\n","# Note that loss value can differ in your instance \n","# due to a different random weight initialization"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(122.6391, device='cuda:0', grad_fn=<MseLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xl8F2ZAsm_ou"},"source":["In the preceding code, mynet(X) calculates the output values when the input is passed through the neural network. Furthermore, the loss_func function calculates the MSELoss value corresponding to the prediction of the neural network (_Y) and the actual values (Y)."]},{"cell_type":"markdown","metadata":{"id":"xciOx1VLoD3L"},"source":["Now that we have defined the loss function, we will define the optimizer that tries to reduce the loss value. The input to the optimizer will be the parameters (weights and biases) corresponding to the neural network and the learning rate when updating the weights.\n","\n","\n","*   For this instance, we will consider the stochastic gradient descent (more on different optimizers and the impact of the learning rate in the next chapter).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"miHU9gBwo6sh"},"source":["# **Import the SGD method from the torch.optim module and then pass the neural network object (mynet) and learning rate (lr) as parameters to the SGD method:**"]},{"cell_type":"code","metadata":{"id":"1p6lkr61mwWf","executionInfo":{"status":"ok","timestamp":1621797184466,"user_tz":-300,"elapsed":428,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["from torch.optim import SGD\n","opt = SGD(mynet.parameters(),lr =0.001)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"egVvLTTQpU7U"},"source":["Perform all the steps to be done in an epoch together:\n","\n","\n","*   Calculate the loss value corresponding to the given input and output.\n","*   Calculate the gradient corresponding to each parameter.\n","\n","\n","1.   Update the weights based on the learning rate and gradient of each parameter.\n","2.   Once the weights are updated, ensure that the gradients that have been calculated in the previous step are flushed before calculating the gradients in the next epoch:\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tGD0wB0ApR1-","executionInfo":{"status":"ok","timestamp":1621797555261,"user_tz":-300,"elapsed":587,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"52b242d0-6c8d-4263-a672-753a72866a5d"},"source":["# NOTE - This line of code is not a part of model building, \n","# this is used only for illustration of how we perform \n","opt.zero_grad()  # flush the previous epoch's gradients\n","loss_value = loss_func(mynet(X),Y) # compute loss\n","loss_value.backward() # perform back-propagation\n","opt.step # update the weights according to the gradients computed"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method SGD.step of SGD (\n","Parameter Group 0\n","    dampening: 0\n","    lr: 0.001\n","    momentum: 0\n","    nesterov: False\n","    weight_decay: 0\n",")>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"oDXv2a2Dq0cR"},"source":["Repeat the preceding steps as many times as the number of epochs using a for loop. In the following example, we are performing the weight update process for a total of 50 epochs. Furthermore, we are storing the loss value in each epoch in the list – loss_history:"]},{"cell_type":"code","metadata":{"id":"d-Bib2mlqj3v","executionInfo":{"status":"ok","timestamp":1621797741755,"user_tz":-300,"elapsed":802,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["loss_history = []\n","for _ in range(50):\n","    opt.zero_grad()\n","    loss_value = loss_func(mynet(X),Y)\n","    loss_value.backward()\n","    opt.step()\n","    loss_history.append(loss_value)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4pn6141jrpLh"},"source":["# Let's plot the variation in loss over increasing epochs (as we saw in the previous chapter, we update weights in such a way that the overall loss value decreases with increasing epochs):"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"uHHogYkBrZsh","executionInfo":{"status":"ok","timestamp":1621797824730,"user_tz":-300,"elapsed":747,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"d341ff44-6610-4d2d-a4c2-d52f7e1720ce"},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.plot(loss_history)\n","plt.title('Loss variation over increasing epochs')\n","plt.xlabel('epochs')\n","plt.ylabel('loss value')"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'loss value')"]},"metadata":{"tags":[]},"execution_count":20},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"ay5LqsO5r1_b"},"source":["Note that, as expected, the loss value decreases over increasing epochs."]},{"cell_type":"markdown","metadata":{"id":"ZKH-rLpHr-dc"},"source":["So far, in this section, we have updated the weights of a neural network by calculating the loss based on all the data points provided in the input dataset."]}]}