{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Specifying_batch_size_while_training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPpPZEKFQ4Z2pH/f4wrBASB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GfYUaAA2yUwy"},"source":["# **Dataset, DataLoader, and batch size**"]},{"cell_type":"markdown","metadata":{"id":"GyERblUJykJH"},"source":["One hyperparameter in a neural network that we have not considered yet is the batch size. Batch size refers to the number of data points considered to calculate the loss value or update weights."]},{"cell_type":"markdown","metadata":{"id":"J3f9zZTYyshx"},"source":["his hyperparameter especially comes in handy in scenarios where there are millions of data points, and using all of them for one instance of weight update is not optimal, as memory is not available to hold so much information. In addition, a sample can be representative enough of the data. Batch size helps in fetching multiple samples of data that are representative enough, but not necessarily 100% representative of the total data."]},{"cell_type":"markdown","metadata":{"id":"TGfidkVlyxWK"},"source":["In this section, we will come up with a way to specify the batch size to be considered when calculating the gradient of weights, to update weights, which is in turn used to calculate the updated loss value"]},{"cell_type":"markdown","metadata":{"id":"9i-GY2DXy7l-"},"source":["# **Import the methods that help in loading data and dealing with datasets:**"]},{"cell_type":"code","metadata":{"id":"Ge3Lkm5nyNXv","executionInfo":{"status":"ok","timestamp":1621802126835,"user_tz":-300,"elapsed":765,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["import torch\n","from torch.utils.data import Dataset,DataLoader\n","import torch\n","import torch.nn as nn"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j28mJPYFzurq"},"source":["# Import the data, convert the data into floating-point numbers, and register them to a device"]},{"cell_type":"code","metadata":{"id":"XZxzl359zlA9","executionInfo":{"status":"ok","timestamp":1621802126841,"user_tz":-300,"elapsed":41,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["x = [[1,2],[3,4],[5,6],[7,8]]\n","y = [[3],[7],[11],[15]]"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AosI1FaE0K2c"},"source":["# Convert the data into floating-point numbers"]},{"cell_type":"code","metadata":{"id":"7dT4Mucqzp9f","executionInfo":{"status":"ok","timestamp":1621802126847,"user_tz":-300,"elapsed":44,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["X = torch.tensor(x).float()\n","Y = torch.tensor(y).float()"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w3TOl_0j0dHJ"},"source":["# Register data to the device – given that we are working on a GPU, we specify that the device is 'cuda'. "]},{"cell_type":"code","metadata":{"id":"bTwY7_IE0VbG","executionInfo":{"status":"ok","timestamp":1621802130059,"user_tz":-300,"elapsed":3252,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["device = 'cuda' if torch.cuda.is_available() else 'gpu'\n","X = X.to(device)\n","Y = Y.to(device)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m1b5_8Ha1hVI"},"source":["# Instantiate a class of the dataset – MyDataset:"]},{"cell_type":"markdown","metadata":{"id":"DLdGaGFo3AQ1"},"source":["Within the MyDataset class, we store the information to fetch one data point at a time so that a batch of data points can be bundled together (using DataLoader) and be sent through one forward and one back-propagation in order to update the weights:\n","\n","\n","*   Define an __init__ method that takes input and output pairs and converts them into Torch float objects:\n","*   Specify the length (__len__) of the input dataset:\n","\n","\n","*   Finally, the __getitem__ method is used to fetch a specific row:\n","\n","*   In the preceding code, ix refers to the index of the row that is to be fetched from the dataset.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RVdrp_yz1lBQ","executionInfo":{"status":"ok","timestamp":1621802130062,"user_tz":-300,"elapsed":67,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"93a2ac80-bed0-48b5-d308-777409af0614"},"source":["class MyDataset(Dataset):\n","    def __init__(self,x,y):\n","        self.x = torch.tensor(x).float()\n","        self.y = torch.tensor(y).float()\n","    def __len__(self):\n","        return len(self.x)\n","    def __getitem__(self, ix):\n","        return self.x[ix], self.y[ix]\n","ds = MyDataset(X, Y)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"zj4tzEsN5O0I"},"source":["# Pass the dataset instance defined previously through DataLoader to fetch the batch_size number of data points from the original input and output tensor objects:"]},{"cell_type":"code","metadata":{"id":"NQ6rxqRc4zUb","executionInfo":{"status":"ok","timestamp":1621802130064,"user_tz":-300,"elapsed":38,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["dl = DataLoader(ds, batch_size=2, shuffle=True)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wcCoZ4RF6DK5"},"source":["# **To fetch the batches from dl, we loop through it:**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObPHnO405KhM","executionInfo":{"status":"ok","timestamp":1621802130613,"user_tz":-300,"elapsed":580,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"2c3c00e9-a08a-4db6-d2db-845eb13168b7"},"source":["# NOTE - This line of code is not a part of model building, \n","# this is used only for illustration of \n","# how to print the input and output batches of data\n","for x,y in dl:\n","    print(x,y)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["tensor([[7., 8.],\n","        [5., 6.]], device='cuda:0') tensor([[15.],\n","        [11.]], device='cuda:0')\n","tensor([[3., 4.],\n","        [1., 2.]], device='cuda:0') tensor([[7.],\n","        [3.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FoAhnc0z7knI"},"source":["# Now, we define the neural network class as we defined in the previous section:"]},{"cell_type":"code","metadata":{"id":"qGLn8xGs6Ig_","executionInfo":{"status":"ok","timestamp":1621802130619,"user_tz":-300,"elapsed":45,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["class MyNeuralNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.input_to_hidden_layer = nn.Linear(2,8)\n","        self.hidden_layer_activation = nn.ReLU()\n","        self.hidden_to_output_layer = nn.Linear(8,1)\n","    def forward(self, x):\n","        x = self.input_to_hidden_layer(x)\n","        x = self.hidden_layer_activation(x)\n","        x = self.hidden_to_output_layer(x)\n","        return x"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0cf3e7lF7vET"},"source":["# Next, we define the model object (mynet), loss function (loss_func), and optimizer (opt) too, as defined in the previous section:"]},{"cell_type":"code","metadata":{"id":"eX7aHgnI7w0y","executionInfo":{"status":"ok","timestamp":1621802130622,"user_tz":-300,"elapsed":43,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["mynet = MyNeuralNet().to(device)\n","loss_func = nn.MSELoss()\n","from torch.optim import SGD\n","opt = SGD(mynet.parameters(), lr = 0.001)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bWQ0VVIP8v1O"},"source":["# Finally, loop through the batches of data points to minimize the loss value, just like we did in step 6 in the previous section:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UPgY2S6Z71p2","executionInfo":{"status":"ok","timestamp":1621802314397,"user_tz":-300,"elapsed":490,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"abc9dac2-5b54-409b-f873-fe54ace2bda2"},"source":["import time\n","loss_history = []\n","start = time.time()\n","for _ in range(50):\n","    for data in dl:\n","        x, y = data\n","        opt.zero_grad()\n","        loss_value = loss_func(mynet(x),y)\n","        loss_value.backward()\n","        opt.step()\n","        loss_history.append(loss_value)\n","end = time.time()\n","print(end - start)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["0.10752296447753906\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b9ZH8lPS82WM","executionInfo":{"status":"ok","timestamp":1621802361412,"user_tz":-300,"elapsed":435,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["val_x = [[10,11]]"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkjk-7BX9B5X","executionInfo":{"status":"ok","timestamp":1621802404215,"user_tz":-300,"elapsed":430,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["val_x = torch.tensor(val_x).float().to(device)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-2AbyjlU9MUT","executionInfo":{"status":"ok","timestamp":1621802448437,"user_tz":-300,"elapsed":439,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"b9c649a5-45c1-47ff-bef6-6abbc7471c77"},"source":["mynet(val_x)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[20.1701]], device='cuda:0', grad_fn=<AddmmBackward>)"]},"metadata":{"tags":[]},"execution_count":13}]}]}