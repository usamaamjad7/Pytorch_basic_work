{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Operations_on_tensor.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO2XbgAIkOj0BhfWCfh70O2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"61fHj2isl3-j"},"source":["# **Operations on Tensor**"]},{"cell_type":"markdown","metadata":{"id":"Y074Cf40qsaW"},"source":["\n","Similar to Numpy,you can perform various basics operations on tensor objects.Parallel to neural network work operations are the matrix multiplication of input with weights the addition of bias terms and reshaping input or weight values when required..\n"]},{"cell_type":"markdown","metadata":{"id":"EweCgygInoWv"},"source":["# Multiplication of all elements present in x by 10 can be performed using the following code.."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OqidBGKUm_lA","executionInfo":{"status":"ok","timestamp":1621748118780,"user_tz":-300,"elapsed":567,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"c684dc07-6580-4df0-a284-ec7ed25c18cb"},"source":["import torch\n","x = torch.tensor([[1,2,3,4],[5,6,7,8]])\n","print(x*10)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["tensor([[10, 20, 30, 40],\n","        [50, 60, 70, 80]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iqR0LWRqoWZC"},"source":["## Adding 10 to the elements in x and storing the resulting tensor in y can be performed using the following code"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UTKKLejZoNDa","executionInfo":{"status":"ok","timestamp":1621748119296,"user_tz":-300,"elapsed":28,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"8aaa4c4e-ef57-45d5-a150-0030ecac548d"},"source":["x = torch.tensor([[1,2,3,4],[5,6,7,8]])\n","y = x.add(10)\n","print(y)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["tensor([[11, 12, 13, 14],\n","        [15, 16, 17, 18]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l0aw_nFophZn"},"source":["# Reshaping a tensor can be performed using the following code"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yk-SJ8cZpK_B","executionInfo":{"status":"ok","timestamp":1621748119301,"user_tz":-300,"elapsed":27,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"382f6ae9-c60f-4035-f0fc-ef6b77df1f04"},"source":["y = torch.tensor([2,3,1,0])\n","y = y.view(4,1)\n","print(y)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["tensor([[2],\n","        [3],\n","        [1],\n","        [0]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tZPNHmpUsPkv"},"source":["\n","\n","Another way to reshape a tensor is by using the squeeze method, \n","  torch.squeeze() returns a tensor with all the dimensions of input of size \"1\" removed.. **\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IdzGAm57rfOQ","executionInfo":{"status":"ok","timestamp":1621748257829,"user_tz":-300,"elapsed":456,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"a687d4d4-c315-4bef-abe6-681115fc27e2"},"source":["x = torch.randn(10,1,10)\n","z1 = torch.squeeze(x,1) # similar to np.squeeze(x,1)\n","# The same operations can be directly performed..\n","# x by calling squeeze and the dimension to squeeze out\n","\n","z2 = x.squeeze(1)\n","# All the elements in both tensor are equal..\n","print('Squeeze:\\n', x.shape,z1.shape)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Squeeze:\n"," torch.Size([10, 1, 10]) torch.Size([10, 10])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8FlkJ1FhvoWR"},"source":["# ***Another way to using  torch.squeeze ***"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tXXRVNMHtadX","executionInfo":{"status":"ok","timestamp":1621748262900,"user_tz":-300,"elapsed":557,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"24fcac2a-5678-4323-957f-cc3fe7acbbdf"},"source":["x = torch.randn(1,2,1,2,1,1,2)\n","y= torch.squeeze(x)\n","print('Squeeze:\\n', x.shape,y.shape)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Squeeze:\n"," torch.Size([1, 2, 1, 2, 1, 1, 2]) torch.Size([2, 2, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ix_JC7T_v988"},"source":["# The opposite of squeeze is unsqueeze, which means we add a dimension to the matrix, which can be performed using the following code"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eigUH0gltbew","executionInfo":{"status":"ok","timestamp":1621748269993,"user_tz":-300,"elapsed":1176,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"d89ae12e-c33a-4e57-b4db-958d9ed41d40"},"source":["x = torch.randn(10,10)\n","print(x.shape)\n","#torch.size(10,10)\n","z1 = x.unsqueeze(0)\n","print(z1.shape)\n","#torch.size(1,10,10)\n","# the same can be acheived using [None] indexing\n","## Adding None will auto create a fake dim at the\n","# specified axis\n","\n","x = torch.randn(10,10)\n","z2,z3,z4 = x[None],x[:,None],x[:,:,None]\n","print(z2.shape,z3.shape,z4.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["torch.Size([10, 10])\n","torch.Size([1, 10, 10])\n","torch.Size([1, 10, 10]) torch.Size([10, 1, 10]) torch.Size([10, 10, 1])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MVk-RhWQyHkj"},"source":["Using None for indexing is a fancy way of unsqueezing, as shown, and will be used often in this book for creating fake channel/batch dimensions."]},{"cell_type":"markdown","metadata":{"id":"U8-91XhCyYAO"},"source":["# Matrix Multiplication of two different tensors can be performed using the following code.."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XSbOit09xxwk","executionInfo":{"status":"ok","timestamp":1621748273255,"user_tz":-300,"elapsed":371,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"2944917e-aac8-4b24-cd74-1ca286526bc6"},"source":["x = torch.tensor([1,2,3,4])\n","y = torch.tensor([5,6,7,8])\n","print(torch.matmul(x, y))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["tensor(70)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qBENETWxy3hG","executionInfo":{"status":"ok","timestamp":1621748273919,"user_tz":-300,"elapsed":3,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"09594df1-9e88-4a55-8a79-f64c7cc3930d"},"source":["print(x@y)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["tensor(70)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fgoFGQ60iFXh"},"source":["# Similar to concatenate in Numpy ,we can perform concatenation of tensors using cat method..."]},{"cell_type":"code","metadata":{"id":"mhS7-AJKz4SE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621748277127,"user_tz":-300,"elapsed":348,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"bb707573-b654-4f1a-e41d-32700cd7c78a"},"source":["import torch\n","x= torch.randn(10,10,10)\n","z = torch.cat([x,x],axis=0) # similar to np.concatenate()\n","print(\"cat axis 0\",x.shape,z.shape)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["cat axis 0 torch.Size([10, 10, 10]) torch.Size([20, 10, 10])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DJmCu_FLjGDs"},"source":["On index One..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCDNixEVi9xw","executionInfo":{"status":"ok","timestamp":1621748279850,"user_tz":-300,"elapsed":476,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"bcafa2b8-ee13-4242-91ef-7e3c4449fccc"},"source":["z =torch.cat([x,x],axis=1)\n","print(\"cat axis\",x.shape,z.shape )"],"execution_count":12,"outputs":[{"output_type":"stream","text":["cat axis torch.Size([10, 10, 10]) torch.Size([10, 20, 10])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-AvSDcQmmAOh"},"source":["# **Extraction of the maximum value in a tensor can be performed using the following code:**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"If-K97Wejaf4","executionInfo":{"status":"ok","timestamp":1621748281752,"user_tz":-300,"elapsed":17,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"604464e3-b94e-404c-97cc-f866e0aeb685"},"source":["x = torch.arange(25).reshape(5,5)\n","print(\"max :\",x.shape,x.max())"],"execution_count":13,"outputs":[{"output_type":"stream","text":["max : torch.Size([5, 5]) tensor(24)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nmtS6U7Vmk99"},"source":["# We can extract the maximum value along with row index where the maximum value will be present..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DADQ3usklUHN","executionInfo":{"status":"ok","timestamp":1621748284368,"user_tz":-300,"elapsed":705,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"1d7cc0db-0f4a-41ee-c4e6-740fc33f2d74"},"source":["x.max(0)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.return_types.max(values=tensor([20, 21, 22, 23, 24]), indices=tensor([4, 4, 4, 4, 4]))"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"6TI7hAepoYPU"},"source":["Note that, in the preceding output, we are fetching the maximum values across dimension 0, which is the rows of the tensor. Hence, the maximum values across all rows are the values present in the 4th index and hence the indices output is all fours too. Furthermore, .max returns both the maximum values and the location (argmax) of the maximum values."]},{"cell_type":"markdown","metadata":{"id":"QiOAQT0XoaR6"},"source":["# Similarly the output when fetching the maximum value across columns is as follows.."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VBAphdJama-Y","executionInfo":{"status":"ok","timestamp":1621748287460,"user_tz":-300,"elapsed":615,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"24241a78-5730-457c-a1b9-e3a856fa7f29"},"source":["m=x.max(1)\n","print(\"max in axis 1: \\n\",m)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["max in axis 1: \n"," torch.return_types.max(\n","values=tensor([ 4,  9, 14, 19, 24]),\n","indices=tensor([4, 4, 4, 4, 4]))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nyHsPGDOo4f8","executionInfo":{"status":"ok","timestamp":1621748288042,"user_tz":-300,"elapsed":10,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"459168da-e210-4529-e881-1cb03a087679"},"source":["m,arg = x.max(1)\n","print('max in axis 1 :\\n',m,arg)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["max in axis 1 :\n"," tensor([ 4,  9, 14, 19, 24]) tensor([4, 4, 4, 4, 4])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m_ZR4jh6pah8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621748289149,"user_tz":-300,"elapsed":6,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"543d5b83-7ea9-4bc5-9366-51b40df182ae"},"source":["x = torch.randn(10,20,30)\n","z = x.permute(2,0,1)\n","print('Permute dimensions:', x.shape, z.shape)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Permute dimensions: torch.Size([10, 20, 30]) torch.Size([30, 10, 20])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a2okuVevtg6b"},"source":["Note that the shape of the tensor changes when we perform permute on top of the original tensor.\n","\n","\n","*   Never reshape (that is, use tensor.view on) a tensor to swap the dimensions. Even though Torch will not throw an error, this is wrong and will create unforeseen results during training. If you need to swap dimensions, always use permute.\n","\n"]},{"cell_type":"code","metadata":{"id":"hm1wXPTlqrFz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621748347217,"user_tz":-300,"elapsed":378,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"2838d4af-ea6d-4392-8193-14f41110906a"},"source":["dir(torch.Tensor)"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['T',\n"," '__abs__',\n"," '__add__',\n"," '__and__',\n"," '__array__',\n"," '__array_priority__',\n"," '__array_wrap__',\n"," '__bool__',\n"," '__class__',\n"," '__complex__',\n"," '__contains__',\n"," '__cuda_array_interface__',\n"," '__deepcopy__',\n"," '__delattr__',\n"," '__delitem__',\n"," '__dict__',\n"," '__dir__',\n"," '__div__',\n"," '__doc__',\n"," '__eq__',\n"," '__float__',\n"," '__floordiv__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__getitem__',\n"," '__gt__',\n"," '__hash__',\n"," '__iadd__',\n"," '__iand__',\n"," '__idiv__',\n"," '__ifloordiv__',\n"," '__ilshift__',\n"," '__imod__',\n"," '__imul__',\n"," '__index__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__int__',\n"," '__invert__',\n"," '__ior__',\n"," '__ipow__',\n"," '__irshift__',\n"," '__isub__',\n"," '__iter__',\n"," '__itruediv__',\n"," '__ixor__',\n"," '__le__',\n"," '__len__',\n"," '__long__',\n"," '__lshift__',\n"," '__lt__',\n"," '__matmul__',\n"," '__mod__',\n"," '__module__',\n"," '__mul__',\n"," '__ne__',\n"," '__neg__',\n"," '__new__',\n"," '__nonzero__',\n"," '__or__',\n"," '__pow__',\n"," '__radd__',\n"," '__rdiv__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__reversed__',\n"," '__rfloordiv__',\n"," '__rmul__',\n"," '__rpow__',\n"," '__rshift__',\n"," '__rsub__',\n"," '__rtruediv__',\n"," '__setattr__',\n"," '__setitem__',\n"," '__setstate__',\n"," '__sizeof__',\n"," '__str__',\n"," '__sub__',\n"," '__subclasshook__',\n"," '__torch_function__',\n"," '__truediv__',\n"," '__weakref__',\n"," '__xor__',\n"," '_backward_hooks',\n"," '_base',\n"," '_cdata',\n"," '_coalesced_',\n"," '_dimI',\n"," '_dimV',\n"," '_grad',\n"," '_grad_fn',\n"," '_indices',\n"," '_is_view',\n"," '_make_subclass',\n"," '_nnz',\n"," '_reduce_ex_internal',\n"," '_update_names',\n"," '_values',\n"," '_version',\n"," 'abs',\n"," 'abs_',\n"," 'absolute',\n"," 'absolute_',\n"," 'acos',\n"," 'acos_',\n"," 'acosh',\n"," 'acosh_',\n"," 'add',\n"," 'add_',\n"," 'addbmm',\n"," 'addbmm_',\n"," 'addcdiv',\n"," 'addcdiv_',\n"," 'addcmul',\n"," 'addcmul_',\n"," 'addmm',\n"," 'addmm_',\n"," 'addmv',\n"," 'addmv_',\n"," 'addr',\n"," 'addr_',\n"," 'align_as',\n"," 'align_to',\n"," 'all',\n"," 'allclose',\n"," 'amax',\n"," 'amin',\n"," 'angle',\n"," 'any',\n"," 'apply_',\n"," 'arccos',\n"," 'arccos_',\n"," 'arccosh',\n"," 'arccosh_',\n"," 'arcsin',\n"," 'arcsin_',\n"," 'arcsinh',\n"," 'arcsinh_',\n"," 'arctan',\n"," 'arctan_',\n"," 'arctanh',\n"," 'arctanh_',\n"," 'argmax',\n"," 'argmin',\n"," 'argsort',\n"," 'as_strided',\n"," 'as_strided_',\n"," 'as_subclass',\n"," 'asin',\n"," 'asin_',\n"," 'asinh',\n"," 'asinh_',\n"," 'atan',\n"," 'atan2',\n"," 'atan2_',\n"," 'atan_',\n"," 'atanh',\n"," 'atanh_',\n"," 'backward',\n"," 'baddbmm',\n"," 'baddbmm_',\n"," 'bernoulli',\n"," 'bernoulli_',\n"," 'bfloat16',\n"," 'bincount',\n"," 'bitwise_and',\n"," 'bitwise_and_',\n"," 'bitwise_not',\n"," 'bitwise_not_',\n"," 'bitwise_or',\n"," 'bitwise_or_',\n"," 'bitwise_xor',\n"," 'bitwise_xor_',\n"," 'bmm',\n"," 'bool',\n"," 'broadcast_to',\n"," 'byte',\n"," 'cauchy_',\n"," 'ceil',\n"," 'ceil_',\n"," 'char',\n"," 'cholesky',\n"," 'cholesky_inverse',\n"," 'cholesky_solve',\n"," 'chunk',\n"," 'clamp',\n"," 'clamp_',\n"," 'clamp_max',\n"," 'clamp_max_',\n"," 'clamp_min',\n"," 'clamp_min_',\n"," 'clip',\n"," 'clip_',\n"," 'clone',\n"," 'coalesce',\n"," 'conj',\n"," 'contiguous',\n"," 'copy_',\n"," 'copysign',\n"," 'copysign_',\n"," 'cos',\n"," 'cos_',\n"," 'cosh',\n"," 'cosh_',\n"," 'count_nonzero',\n"," 'cpu',\n"," 'cross',\n"," 'cuda',\n"," 'cummax',\n"," 'cummin',\n"," 'cumprod',\n"," 'cumprod_',\n"," 'cumsum',\n"," 'cumsum_',\n"," 'data',\n"," 'data_ptr',\n"," 'deg2rad',\n"," 'deg2rad_',\n"," 'dense_dim',\n"," 'dequantize',\n"," 'det',\n"," 'detach',\n"," 'detach_',\n"," 'device',\n"," 'diag',\n"," 'diag_embed',\n"," 'diagflat',\n"," 'diagonal',\n"," 'diff',\n"," 'digamma',\n"," 'digamma_',\n"," 'dim',\n"," 'dist',\n"," 'div',\n"," 'div_',\n"," 'divide',\n"," 'divide_',\n"," 'dot',\n"," 'double',\n"," 'dtype',\n"," 'eig',\n"," 'element_size',\n"," 'eq',\n"," 'eq_',\n"," 'equal',\n"," 'erf',\n"," 'erf_',\n"," 'erfc',\n"," 'erfc_',\n"," 'erfinv',\n"," 'erfinv_',\n"," 'exp',\n"," 'exp2',\n"," 'exp2_',\n"," 'exp_',\n"," 'expand',\n"," 'expand_as',\n"," 'expm1',\n"," 'expm1_',\n"," 'exponential_',\n"," 'fill_',\n"," 'fill_diagonal_',\n"," 'fix',\n"," 'fix_',\n"," 'flatten',\n"," 'flip',\n"," 'fliplr',\n"," 'flipud',\n"," 'float',\n"," 'float_power',\n"," 'float_power_',\n"," 'floor',\n"," 'floor_',\n"," 'floor_divide',\n"," 'floor_divide_',\n"," 'fmax',\n"," 'fmin',\n"," 'fmod',\n"," 'fmod_',\n"," 'frac',\n"," 'frac_',\n"," 'gather',\n"," 'gcd',\n"," 'gcd_',\n"," 'ge',\n"," 'ge_',\n"," 'geometric_',\n"," 'geqrf',\n"," 'ger',\n"," 'get_device',\n"," 'grad',\n"," 'grad_fn',\n"," 'greater',\n"," 'greater_',\n"," 'greater_equal',\n"," 'greater_equal_',\n"," 'gt',\n"," 'gt_',\n"," 'half',\n"," 'hardshrink',\n"," 'has_names',\n"," 'heaviside',\n"," 'heaviside_',\n"," 'histc',\n"," 'hypot',\n"," 'hypot_',\n"," 'i0',\n"," 'i0_',\n"," 'igamma',\n"," 'igamma_',\n"," 'igammac',\n"," 'igammac_',\n"," 'imag',\n"," 'index_add',\n"," 'index_add_',\n"," 'index_copy',\n"," 'index_copy_',\n"," 'index_fill',\n"," 'index_fill_',\n"," 'index_put',\n"," 'index_put_',\n"," 'index_select',\n"," 'indices',\n"," 'inner',\n"," 'int',\n"," 'int_repr',\n"," 'inverse',\n"," 'is_coalesced',\n"," 'is_complex',\n"," 'is_contiguous',\n"," 'is_cuda',\n"," 'is_distributed',\n"," 'is_floating_point',\n"," 'is_leaf',\n"," 'is_meta',\n"," 'is_mkldnn',\n"," 'is_nonzero',\n"," 'is_pinned',\n"," 'is_quantized',\n"," 'is_same_size',\n"," 'is_set_to',\n"," 'is_shared',\n"," 'is_signed',\n"," 'is_sparse',\n"," 'is_vulkan',\n"," 'is_xpu',\n"," 'isclose',\n"," 'isfinite',\n"," 'isinf',\n"," 'isnan',\n"," 'isneginf',\n"," 'isposinf',\n"," 'isreal',\n"," 'istft',\n"," 'item',\n"," 'kron',\n"," 'kthvalue',\n"," 'layout',\n"," 'lcm',\n"," 'lcm_',\n"," 'ldexp',\n"," 'ldexp_',\n"," 'le',\n"," 'le_',\n"," 'lerp',\n"," 'lerp_',\n"," 'less',\n"," 'less_',\n"," 'less_equal',\n"," 'less_equal_',\n"," 'lgamma',\n"," 'lgamma_',\n"," 'log',\n"," 'log10',\n"," 'log10_',\n"," 'log1p',\n"," 'log1p_',\n"," 'log2',\n"," 'log2_',\n"," 'log_',\n"," 'log_normal_',\n"," 'log_softmax',\n"," 'logaddexp',\n"," 'logaddexp2',\n"," 'logcumsumexp',\n"," 'logdet',\n"," 'logical_and',\n"," 'logical_and_',\n"," 'logical_not',\n"," 'logical_not_',\n"," 'logical_or',\n"," 'logical_or_',\n"," 'logical_xor',\n"," 'logical_xor_',\n"," 'logit',\n"," 'logit_',\n"," 'logsumexp',\n"," 'long',\n"," 'lstsq',\n"," 'lt',\n"," 'lt_',\n"," 'lu',\n"," 'lu_solve',\n"," 'map2_',\n"," 'map_',\n"," 'masked_fill',\n"," 'masked_fill_',\n"," 'masked_scatter',\n"," 'masked_scatter_',\n"," 'masked_select',\n"," 'matmul',\n"," 'matrix_exp',\n"," 'matrix_power',\n"," 'max',\n"," 'maximum',\n"," 'mean',\n"," 'median',\n"," 'min',\n"," 'minimum',\n"," 'mm',\n"," 'mode',\n"," 'moveaxis',\n"," 'movedim',\n"," 'msort',\n"," 'mul',\n"," 'mul_',\n"," 'multinomial',\n"," 'multiply',\n"," 'multiply_',\n"," 'mv',\n"," 'mvlgamma',\n"," 'mvlgamma_',\n"," 'name',\n"," 'names',\n"," 'nan_to_num',\n"," 'nan_to_num_',\n"," 'nanmedian',\n"," 'nanquantile',\n"," 'nansum',\n"," 'narrow',\n"," 'narrow_copy',\n"," 'ndim',\n"," 'ndimension',\n"," 'ne',\n"," 'ne_',\n"," 'neg',\n"," 'neg_',\n"," 'negative',\n"," 'negative_',\n"," 'nelement',\n"," 'new',\n"," 'new_empty',\n"," 'new_empty_strided',\n"," 'new_full',\n"," 'new_ones',\n"," 'new_tensor',\n"," 'new_zeros',\n"," 'nextafter',\n"," 'nextafter_',\n"," 'nonzero',\n"," 'norm',\n"," 'normal_',\n"," 'not_equal',\n"," 'not_equal_',\n"," 'numel',\n"," 'numpy',\n"," 'orgqr',\n"," 'ormqr',\n"," 'outer',\n"," 'output_nr',\n"," 'permute',\n"," 'pin_memory',\n"," 'pinverse',\n"," 'polygamma',\n"," 'polygamma_',\n"," 'pow',\n"," 'pow_',\n"," 'prelu',\n"," 'prod',\n"," 'put_',\n"," 'q_per_channel_axis',\n"," 'q_per_channel_scales',\n"," 'q_per_channel_zero_points',\n"," 'q_scale',\n"," 'q_zero_point',\n"," 'qr',\n"," 'qscheme',\n"," 'quantile',\n"," 'rad2deg',\n"," 'rad2deg_',\n"," 'random_',\n"," 'ravel',\n"," 'real',\n"," 'reciprocal',\n"," 'reciprocal_',\n"," 'record_stream',\n"," 'refine_names',\n"," 'register_hook',\n"," 'reinforce',\n"," 'relu',\n"," 'relu_',\n"," 'remainder',\n"," 'remainder_',\n"," 'rename',\n"," 'rename_',\n"," 'renorm',\n"," 'renorm_',\n"," 'repeat',\n"," 'repeat_interleave',\n"," 'requires_grad',\n"," 'requires_grad_',\n"," 'reshape',\n"," 'reshape_as',\n"," 'resize',\n"," 'resize_',\n"," 'resize_as',\n"," 'resize_as_',\n"," 'retain_grad',\n"," 'roll',\n"," 'rot90',\n"," 'round',\n"," 'round_',\n"," 'rsqrt',\n"," 'rsqrt_',\n"," 'scatter',\n"," 'scatter_',\n"," 'scatter_add',\n"," 'scatter_add_',\n"," 'select',\n"," 'set_',\n"," 'sgn',\n"," 'sgn_',\n"," 'shape',\n"," 'share_memory_',\n"," 'short',\n"," 'sigmoid',\n"," 'sigmoid_',\n"," 'sign',\n"," 'sign_',\n"," 'signbit',\n"," 'sin',\n"," 'sin_',\n"," 'sinc',\n"," 'sinc_',\n"," 'sinh',\n"," 'sinh_',\n"," 'size',\n"," 'slogdet',\n"," 'smm',\n"," 'softmax',\n"," 'solve',\n"," 'sort',\n"," 'sparse_dim',\n"," 'sparse_mask',\n"," 'sparse_resize_',\n"," 'sparse_resize_and_clear_',\n"," 'split',\n"," 'split_with_sizes',\n"," 'sqrt',\n"," 'sqrt_',\n"," 'square',\n"," 'square_',\n"," 'squeeze',\n"," 'squeeze_',\n"," 'sspaddmm',\n"," 'std',\n"," 'stft',\n"," 'storage',\n"," 'storage_offset',\n"," 'storage_type',\n"," 'stride',\n"," 'sub',\n"," 'sub_',\n"," 'subtract',\n"," 'subtract_',\n"," 'sum',\n"," 'sum_to_size',\n"," 'svd',\n"," 'swapaxes',\n"," 'swapaxes_',\n"," 'swapdims',\n"," 'swapdims_',\n"," 'symeig',\n"," 't',\n"," 't_',\n"," 'take',\n"," 'tan',\n"," 'tan_',\n"," 'tanh',\n"," 'tanh_',\n"," 'tensor_split',\n"," 'tile',\n"," 'to',\n"," 'to_dense',\n"," 'to_mkldnn',\n"," 'to_sparse',\n"," 'tolist',\n"," 'topk',\n"," 'trace',\n"," 'transpose',\n"," 'transpose_',\n"," 'triangular_solve',\n"," 'tril',\n"," 'tril_',\n"," 'triu',\n"," 'triu_',\n"," 'true_divide',\n"," 'true_divide_',\n"," 'trunc',\n"," 'trunc_',\n"," 'type',\n"," 'type_as',\n"," 'unbind',\n"," 'unflatten',\n"," 'unfold',\n"," 'uniform_',\n"," 'unique',\n"," 'unique_consecutive',\n"," 'unsafe_chunk',\n"," 'unsafe_split',\n"," 'unsafe_split_with_sizes',\n"," 'unsqueeze',\n"," 'unsqueeze_',\n"," 'values',\n"," 'var',\n"," 'vdot',\n"," 'view',\n"," 'view_as',\n"," 'volatile',\n"," 'where',\n"," 'xlogy',\n"," 'xlogy_',\n"," 'xpu',\n"," 'zero_']"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"iNgJ40tmqrpg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621748334986,"user_tz":-300,"elapsed":388,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"4a7a5215-1f45-478c-cf98-04d4300e1612"},"source":["help(torch.Tensor.view)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Help on method_descriptor:\n","\n","view(...)\n","    view(*shape) -> Tensor\n","    \n","    Returns a new tensor with the same data as the :attr:`self` tensor but of a\n","    different :attr:`shape`.\n","    \n","    The returned tensor shares the same data and must have the same number\n","    of elements, but may have a different size. For a tensor to be viewed, the new\n","    view size must be compatible with its original size and stride, i.e., each new\n","    view dimension must either be a subspace of an original dimension, or only span\n","    across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n","    contiguity-like condition that :math:`\\forall i = d, \\dots, d+k-1`,\n","    \n","    .. math::\n","    \n","      \\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n","    \n","    Otherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape`\n","    without copying it (e.g., via :meth:`contiguous`). When it is unclear whether a\n","    :meth:`view` can be performed, it is advisable to use :meth:`reshape`, which\n","    returns a view if the shapes are compatible, and copies (equivalent to calling\n","    :meth:`contiguous`) otherwise.\n","    \n","    Args:\n","        shape (torch.Size or int...): the desired size\n","    \n","    Example::\n","    \n","        >>> x = torch.randn(4, 4)\n","        >>> x.size()\n","        torch.Size([4, 4])\n","        >>> y = x.view(16)\n","        >>> y.size()\n","        torch.Size([16])\n","        >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n","        >>> z.size()\n","        torch.Size([2, 8])\n","    \n","        >>> a = torch.randn(1, 2, 3, 4)\n","        >>> a.size()\n","        torch.Size([1, 2, 3, 4])\n","        >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n","        >>> b.size()\n","        torch.Size([1, 3, 2, 4])\n","        >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n","        >>> c.size()\n","        torch.Size([1, 3, 2, 4])\n","        >>> torch.equal(b, c)\n","        False\n","    \n","    \n","    .. function:: view(dtype) -> Tensor\n","    \n","    Returns a new tensor with the same data as the :attr:`self` tensor but of a\n","    different :attr:`dtype`. :attr:`dtype` must have the same number of bytes per\n","    element as :attr:`self`'s dtype.\n","    \n","    .. warning::\n","    \n","        This overload is not supported by TorchScript, and using it in a Torchscript\n","        program will cause undefined behavior.\n","    \n","    \n","    Args:\n","        dtype (:class:`torch.dtype`): the desired dtype\n","    \n","    Example::\n","    \n","        >>> x = torch.randn(4, 4)\n","        >>> x\n","        tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n","                [-0.1520,  0.7472,  0.5617, -0.8649],\n","                [-2.4724, -0.0334, -0.2976, -0.8499],\n","                [-0.2109,  1.9913, -0.9607, -0.6123]])\n","        >>> x.dtype\n","        torch.float32\n","    \n","        >>> y = x.view(torch.int32)\n","        >>> y\n","        tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n","                [-1105482831,  1061112040,  1057999968, -1084397505],\n","                [-1071760287, -1123489973, -1097310419, -1084649136],\n","                [-1101533110,  1073668768, -1082790149, -1088634448]],\n","            dtype=torch.int32)\n","        >>> y[0, 0] = 1000000000\n","        >>> x\n","        tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n","                [-0.1520,  0.7472,  0.5617, -0.8649],\n","                [-2.4724, -0.0334, -0.2976, -0.8499],\n","                [-0.2109,  1.9913, -0.9607, -0.6123]])\n","    \n","        >>> x.view(torch.int16)\n","        Traceback (most recent call last):\n","          File \"<stdin>\", line 1, in <module>\n","        RuntimeError: Viewing a tensor as a new dtype with a different number of bytes per element is not supported.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ce9UU2ALuBM3","executionInfo":{"status":"aborted","timestamp":1621748119327,"user_tz":-300,"elapsed":42,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":[""],"execution_count":null,"outputs":[]}]}