{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Numpy_Vs_Torch_object_computation_speed_comparison.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP0m13INciHvLNIL+w409Hf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SvU7zWjnOz-W"},"source":["# **Advantages of PyTorch's tensors over NumPy's ndarrays**"]},{"cell_type":"markdown","metadata":{"id":"KBAVP93aO7-N"},"source":["\n","\n","*   In the previous chapter, we saw that when calculating the optimal weight values, we vary each weight by a small amount and understand its impact on reducing the overall loss value. Note that the loss calculation based on the weight update of one weight does not impact the loss calculation of the weight update of other weights in the same iteration. Thus, this process can be optimized if each weight update is being made by a different core in parallel instead of updating weights sequentially. A GPU comes in handy in this scenario as it consists of thousands of cores when compared to a CPU (which, in general, could have <=64 cores).\n","\n","*   A Torch tensor object is optimized to work with a GPU compared to NumPy. To understand this further, let's perform a small experiment, where we perform the operation of matrix multiplication using NumPy arrays in one scenario and tensor objects in another and compare the time taken to perform matrix multiplication in both scenarios:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Lo6zwF9qPS1c"},"source":["# Generate two different torch objects:"]},{"cell_type":"code","metadata":{"id":"fKLHQ-P2ONqd","executionInfo":{"status":"ok","timestamp":1621791698237,"user_tz":-300,"elapsed":5409,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["import torch\n","x = torch.rand(1,6400)\n","y = torch.rand(6400,5000)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yIyGtdrtP6L4"},"source":["# Define the device to which we will store the tensor objects we created in step 1:"]},{"cell_type":"code","metadata":{"id":"gepKXGXQPsKM","executionInfo":{"status":"ok","timestamp":1621791698246,"user_tz":-300,"elapsed":18,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["device = 'cuda' if torch.cuda.is_available() else 'GPU'"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0NEFXHQRSYwJ"},"source":["# Register the tensor objects that were created in step 1 with the device. Registering tensor objects means storing information in a device:"]},{"cell_type":"code","metadata":{"id":"VuVxFoC8Qi4J","executionInfo":{"status":"ok","timestamp":1621791708189,"user_tz":-300,"elapsed":9957,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}}},"source":["x,y = x.to(device),y.to(device)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8eq9BQJMSxlp"},"source":["# Perform matrix multiplication of the Torch objects and also, time it so that we can compare the speed in a scenario where matrix multiplication is performed on NumPy arrays:"]},{"cell_type":"code","metadata":{"id":"geG9QacfSntY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621791738363,"user_tz":-300,"elapsed":30228,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"5515636b-65a4-49a2-e37b-c01b2965f6f6"},"source":["%timeit z=(x@y)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The slowest run took 36.71 times longer than the fastest. This could mean that an intermediate result is being cached.\n","10000 loops, best of 5: 514 Âµs per loop\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_jI09kTOS6qH"},"source":["# Perform matrix multiplication of the same tensors on gpu:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UzJ2Q3AaSstj","executionInfo":{"status":"ok","timestamp":1621792054312,"user_tz":-300,"elapsed":5957,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"722f158a-38b1-4665-a467-2835441f126d"},"source":["x,y = x.cpu(),y.cpu()\n","%timeit z=(x@y)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["100 loops, best of 5: 8.87 ms per loop\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_OSQP0E8ToMq"},"source":["# **Perfom the same matrix multiplication this time on Numpy arrays..**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ubKqWl_FT0bT","executionInfo":{"status":"ok","timestamp":1621792075676,"user_tz":-300,"elapsed":12503,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"9d3bbefc-505f-4d86-e538-73b942e83528"},"source":["import numpy as np\n","x = np.random.random((1,6400))\n","y = np.random.random((6400,5000))\n","%timeit z=np.matmul(x,y)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["100 loops, best of 5: 19.2 ms per loop\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pJxwmVlWWKDl"},"source":["The torch object is faster then Numpy arrays.."]}]}