{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Implementing_custom_loss_function.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM1+kvxHTDcIE6IpVawJjTk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"K0QHJ_dl3ctw"},"source":["# **Implementing a custom loss function**"]},{"cell_type":"markdown","metadata":{"id":"s1aflNSp3krC"},"source":["In certain cases, we might have to implement a loss function that is customized to the problem we are solving â€“ especially in complex use cases involving object detection/generative adversial networks (GANs). PyTorch provides the functionalities for us to build a custom loss function by writing a function of our own."]},{"cell_type":"markdown","metadata":{"id":"Dw_jxa3-3sLa"},"source":["In this section, we will implement a custom loss function that does the same job as that of the MSELoss function that comes pre-built within nn.Module:"]},{"cell_type":"markdown","metadata":{"id":"Dg_hhGcd3_uH"},"source":["# Import the data, build the dataset and DataLoader, and define a neural network, as done in the previous section:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBeu23FGD-RH","executionInfo":{"status":"ok","timestamp":1621837753992,"user_tz":-300,"elapsed":581,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"df805b73-b760-4044-8017-66282ef5e93e"},"source":["\n","\n","x = [[1,2],[3,4],[5,6],[7,8]]\n","y = [[3],[7],[11],[15]]\n","import torch\n","X = torch.tensor(x).float()\n","Y = torch.tensor(y).float()\n","import torch.nn as nn\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","X = X.to(device)\n","Y = Y.to(device) \n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","class MyDataset(Dataset):\n","    def __init__(self,x,y):\n","        self.x = torch.tensor(x).float()\n","        self.y = torch.tensor(y).float()\n","    def __len__(self):\n","        return len(self.x)\n","    def __getitem__(self, ix):\n","        return self.x[ix], self.y[ix]\n","ds = MyDataset(X, Y)\n","dl = DataLoader(ds, batch_size=2, shuffle=True)\n","class MyNeuralNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.input_to_hidden_layer = nn.Linear(2,8)\n","        self.hidden_layer_activation = nn.ReLU()\n","        self.hidden_to_output_layer = nn.Linear(8,1)\n","    def forward(self, x):\n","        x = self.input_to_hidden_layer(x)\n","        x = self.hidden_layer_activation(x)\n","        x = self.hidden_to_output_layer(x)\n","        return x\n","mynet = MyNeuralNet().to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  app.launch_new_instance()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"2_sm8t7mCMyg"},"source":["# Define the custom loss function by taking two tensor objects as input, take their difference, and square them up and return the mean value of the squared difference between the two:"]},{"cell_type":"code","metadata":{"id":"-xzskc_ZBRcC"},"source":["def my_mean_squared_error(_y,y):\n","    loss = (_y-y)**2\n","    loss = loss.mean()\n","    return loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fjs0xL4oCrRo"},"source":["For the same input and output combination that we had in the previous section, nn.MSELoss is used in fetching the mean squared error loss, as follows:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zERnnFyxCj-9","executionInfo":{"status":"ok","timestamp":1621837761114,"user_tz":-300,"elapsed":18,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"a97dfaf7-f3eb-4b09-c9c2-64a190c034d2"},"source":["loss_func = nn.MSELoss()\n","loss_value = loss_func(mynet(X),Y)\n","print(loss_value)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(80.1189, device='cuda:0', grad_fn=<MseLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eH89gxcWEWJD"},"source":["# Similarly, the output of the loss value when we use the function that we defined in step 2 is as follows:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-3uPQyQC3Hv","executionInfo":{"status":"ok","timestamp":1621837901651,"user_tz":-300,"elapsed":512,"user":{"displayName":"Usama Amjad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7L11yS2Fc2bXwrAlwLL_Ih2_lSP1oQy-GZI9wGw=s64","userId":"17999911935826778950"}},"outputId":"319494ae-ad68-4f5c-ee56-0169c572b3db"},"source":["my_mean_squared_error(mynet(X),Y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(80.1189, device='cuda:0', grad_fn=<MeanBackward0>)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"TUYp0P89EauQ"},"source":[""],"execution_count":null,"outputs":[]}]}