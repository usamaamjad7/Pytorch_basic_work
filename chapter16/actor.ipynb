{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "actor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ16-6_c6cu1"
      },
      "source": [
        "\n",
        "import numpy as  np\n",
        "import random\n",
        "from collections import namedtuple,deque\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from model import DQNetworkImageSensor\n",
        "\n",
        "BUFFER_SIZE = int(1e4) # replay buffer size\n",
        "BATCH_SIZE = 32   \n",
        "GAMMA = 0.99\n",
        "TAU = 1e-2\n",
        "LR = 5e-4\n",
        "UPDATE_EVERY = 50\n",
        "ACTION_SIZE = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class Actor():\n",
        "    def __init__(self):\n",
        "        self.qnetwork_local = DQNetworkImageSensor().to(device)\n",
        "        self.qnetwork_target = DQNetworkImageSensor().to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(),lr=LR)\n",
        "\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(ACTION_SIZE,BUFFER_SIZE,BATCH_SIZE,10)\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self,state,action,reward,next_state,done):\n",
        "        # save experience in replay memory\n",
        "        self.memory.add(state,action,reward,next_state,done)\n",
        "\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step ==0:\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                experience = self.memory.sample()\n",
        "                self.learn(experiences,GAMMA)\n",
        "\n",
        "    def act(self,state,eps =0.):\n",
        "        images,lidars,sensors = state['image'], state['lidar'],state['sensor']\n",
        "        images = torch.from_numpy(images).float().unsqueeze(0).to(device)\n",
        "        lidars = torch.from_numpy(lidars).float().unsqueeze(0).to(device)\n",
        "        sensors = torch.from_numpy(sensors).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(images , lidar = lidars,sensor = sensors)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(9))\n",
        "\n",
        "        # return action values\n",
        "\n",
        "    def learn(self,experiences,gamma):\n",
        "        states,actions,rewards,next_states,dones = experiences\n",
        "        images,lidars,sensors = states\n",
        "        next_images,next_lidars,next_sensors = next_states\n",
        "        Q_targets_next = self.qnetwork_target(next_images,lidar=next_lidars, sensor = next_sensors).detach().max(1)[0].unsqueeze(1)\n",
        "        # compute q targets for current states:\n",
        "        Q_targets = rewards + (gamma * Q_target_next *(1-dones))\n",
        "\n",
        "        # Get expected q value from local network\n",
        "        Q_expected = self.qnetwork_local(images,lidar=lidars,sensor = sensors).gather(1,actions.long())\n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        # Minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "        # -------------------------update target_netwokr----------------------#\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target,TAU)\n",
        "\n",
        "    def soft_update(self,local_model,target_model,tau):\n",
        "        for target_param,local_param in zip(target_model.parameters(),local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0 -tau)*target_param.data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  \n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "        \n",
        "    \n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        \n",
        "        images = torch.from_numpy(np.vstack([e.state['image'][None] for e in experiences if e is not None])).float().to(device)\n",
        "        lidars = torch.from_numpy(np.vstack([e.state['lidar'][None] for e in experiences if e is not None])).float().to(device)\n",
        "        sensors = torch.from_numpy(np.vstack([e.state['sensor'] for e in experiences if e is not None])).float().to(device)\n",
        "        states = [images, lidars, sensors]\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_images = torch.from_numpy(np.vstack([e.next_state['image'][None] for e in experiences if e is not None])).float().to(device)\n",
        "        next_lidars = torch.from_numpy(np.vstack([e.next_state['lidar'][None] for e in experiences if e is not None])).float().to(device)\n",
        "        next_sensors = torch.from_numpy(np.vstack([e.next_state['sensor'] for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = [next_images, next_lidars, next_sensors]\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "        \n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}